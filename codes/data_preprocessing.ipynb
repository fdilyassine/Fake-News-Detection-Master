{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " # @author y.fdil\n",
    " # @email y.fdil@edu.umi.ac.ma\n",
    " # @create date 2022-06-12 10:47:16\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Importing the libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def read_tsv(file_path):\n",
    "\n",
    "\n",
    "    dataset = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "\n",
    "\n",
    "    columns_to_select = [1, 2]\n",
    "    dataset = dataset.iloc[:, columns_to_select]\n",
    "    dataset.columns = ['label', 'news']\n",
    "\n",
    "    # Converting the multiclass labels to binary labels\n",
    "    labels_map = {\n",
    "        'true': 'true',\n",
    "        'mostly-true': 'true',\n",
    "        'half-true': 'true',\n",
    "        'false': 'false',\n",
    "        'barely-true': 'false',\n",
    "        'pants-fire': 'false'\n",
    "    }\n",
    "    dataset['label'] = dataset['label'].map(labels_map)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_to_csv(dataset, file_path, file_name):\n",
    "\n",
    "    complete_file_path_with_name = file_path + file_name\n",
    "    dataset.to_csv(complete_file_path_with_name, index=False)\n",
    "\n",
    "\n",
    "def show_dataset_stats(dataset, dataset_name):\n",
    "\n",
    "    print('Statistics of {}'.format(dataset_name))\n",
    "    print('Shape: {}'.format(dataset.shape))\n",
    "    print('Few samples from dataset:')\n",
    "    print(dataset.sample(5))\n",
    "    print()\n",
    "\n",
    "\n",
    "def save_lowercase(dataset, file_path, file_name):\n",
    "\n",
    "    dataset['news'] = dataset['news'].str.lower()\n",
    "    complete_file_path_with_name = file_path+file_name\n",
    "    dataset.to_csv(complete_file_path_with_name, index=False)\n",
    "\n",
    "\n",
    "def show_label_distribution(dataset, dataset_name):\n",
    "\n",
    "    print('Label distribution {}:'.format(dataset_name))\n",
    "    distribution = dataset['label'].value_counts(normalize=True).reset_index()\n",
    "    distribution['label'] = distribution['label']*100\n",
    "    for _, row in distribution.iterrows():\n",
    "        print('{}\\t{:.2f}%'.format(row['index'], row['label']))\n",
    "    print()\n",
    "\n",
    "\n",
    "def create_distribution(dataset, dataset_name):\n",
    "\n",
    "    if not os.path.isdir('../figures'):\n",
    "        os.makedirs('../figures')\n",
    "    sns.countplot(x='label', data=dataset)\n",
    "    plt.title('Label distribution of '+dataset_name)\n",
    "    plt.savefig('../figures/label_distribution_'+dataset_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_dataset_quality(dataset, dataset_name):\n",
    "\n",
    "    print('Checking the quality of {}'.format(dataset_name))\n",
    "    print(dataset.isnull().sum())\n",
    "    print()\n",
    "    number_of_nulls = dataset.isnull().sum().sum()\n",
    "    if number_of_nulls > 0:\n",
    "        dataset.dropna()\n",
    "\n",
    "\n",
    "# File paths of the dataset to be read\n",
    "train_path = '../datasets/train.tsv'\n",
    "valid_path = '../datasets/valid.tsv'\n",
    "test_path = '../datasets/test.tsv'\n",
    "\n",
    "train_data = read_tsv(train_path)\n",
    "valid_data = read_tsv(valid_path)\n",
    "test_data = read_tsv(test_path)\n",
    "\n",
    "# Preprocessing the datasets\n",
    "train_data = preprocess_dataset(train_data)\n",
    "valid_data = preprocess_dataset(valid_data)\n",
    "test_data = preprocess_dataset(test_data)\n",
    "\n",
    "# Path for saving the files\n",
    "save_path = '../datasets/'\n",
    "\n",
    "# Checking the quality of the dataset\n",
    "check_dataset_quality(train_data, 'Train dataset')\n",
    "check_dataset_quality(valid_data, 'Valid dataset')\n",
    "check_dataset_quality(test_data, 'Test dataset')\n",
    "\n",
    "# Saving the datasets in csv format\n",
    "save_to_csv(train_data, save_path, 'train.csv')\n",
    "save_to_csv(valid_data, save_path, 'valid.csv')\n",
    "save_to_csv(test_data, save_path, 'test.csv')\n",
    "\n",
    "# Displaying the stats of the datasets\n",
    "show_dataset_stats(train_data, 'Train dataset')\n",
    "show_dataset_stats(test_data, 'Test dataset')\n",
    "show_dataset_stats(valid_data, 'Valid dataset')\n",
    "\n",
    "# Display the label distribution of the datasets\n",
    "show_label_distribution(train_data, 'Train dataset')\n",
    "show_label_distribution(valid_data, 'Valid dataset')\n",
    "show_label_distribution(test_data, 'Test dataset')\n",
    "\n",
    "# Creating distributions of datasets\n",
    "create_distribution(train_data, 'Train_dataset')\n",
    "create_distribution(valid_data, 'Valid_dataset')\n",
    "create_distribution(test_data, 'Test_dataset')\n",
    "\n",
    "# Saving the datasets with all string in lowercase\n",
    "save_lowercase(train_data, save_path, 'train_lower.csv')\n",
    "save_lowercase(valid_data, save_path, 'valid_lower.csv')\n",
    "save_lowercase(test_data, save_path, 'test_lower.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
